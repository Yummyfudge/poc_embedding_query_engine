# ============================================================================
# Runtime configuration for poc_embedding_query_engin
# Explicitly sourced by the user (no auto-loading)
# Consumed by scripts/answer_with_evidence.py and query runners
# ============================================================================

# ----------------------------------------------------------------------------
# REQUIRED (minimum to run)
# ----------------------------------------------------------------------------
# LITELLM_BASE_URL, LITELLM_API_KEY
# PDF_POC_PG_DSN
# PROJECT_ROOT

# ----------------------------------------------------------------------------
# LiteLLM (proxy)
# ----------------------------------------------------------------------------
LITELLM_BASE_URL=http://127.0.0.1:4000  # example: LiteLLM proxy base URL
LITELLM_API_KEY=CHANGEME  # required

# Prefix added to model names to route requests via LiteLLM proxy (used by answer_with_evidence.py)
LITELLM_MODEL_PREFIX=litellm_proxy/  # required when routing via LiteLLM proxy

# ----------------------------------------------------------------------------
# PostgreSQL DSN
# ----------------------------------------------------------------------------
PDF_POC_PG_DSN="postgresql://DB_USER@DB_HOST:5432/DB_NAME?sslmode=verify-full&sslrootcert=/ABS/PATH/TO/root.crt&sslcert=/ABS/PATH/TO/client.crt&sslkey=/ABS/PATH/TO/client.key"  # required

# ----------------------------------------------------------------------------
# Models
# ----------------------------------------------------------------------------
# Embeddings model exposed by LiteLLM (/embeddings)
EMBEDDINGS_MODEL=local-embed

# Retrieval strategy selection (content query engine)
CONTENT_QUERY_VARIANT=round1

# Chat model exposed by LiteLLM (/v1/chat/completions)
# Default used by the engine if PLANNER_MODEL / ANSWER_MODEL are not set.
CHAT_MODEL=llama_cpp-lane2

# Optional: explicitly split planner vs answer models
PLANNER_MODEL=llama_cpp-lane2
ANSWER_MODEL=llama_cpp-lane2

# ----------------------------------------------------------------------------
# RAG / pipeline settings
# ----------------------------------------------------------------------------
PDF_POC_SCHEMA=pdf_emergency

# Absolute path to the project root (used for resolving relative paths)
PROJECT_ROOT=/ABS/PATH/TO/PROJECT_ROOT

# ----------------------------------------------------------------------------
# Retrieval knobs (env-driven defaults; CLI flags still override)
# ----------------------------------------------------------------------------
RAG_CANDIDATES=120
RAG_TOP_CHUNKS=6
RAG_MAX_PER_PAGE=2
RAG_MIN_SCORE=0.5
RAG_MAX_EXCERPTS_CHARS=4500
RAG_MAX_EXCERPTS=8

# ----------------------------------------------------------------------------
# Retrieval pipeline knobs (Step 2) — extended (mostly unused today)
# Ordered by sub-stage (R2→R5) for clarity. Defaults preserve current behavior.
# ----------------------------------------------------------------------------

# R2 — Query inputs (what gets embedded)
# Future knob. Round-1 currently uses the original query + ALL planner expansions as separate probes.
RAG_QUERY_COUNT=1

# Future knob (reserved). Round-1 currently ignores this and uses user + planner probes.
RAG_QUERY_MODE=USER_ONLY  # UNUSED

# Dedupe near-duplicate planner queries before embedding (reserved)
RAG_QUERY_DEDUP=1  # UNUSED

# Optional fixed prefix/suffix applied to each query string (reserved)
RAG_QUERY_PREFIX=  # UNUSED
RAG_QUERY_SUFFIX=  # UNUSED

# R2.5 — Embedding behavior (Infinity/LiteLLM)
# Normalize embeddings before similarity (recommended; current code normalizes)
RAG_EMBED_NORMALIZE=1  # UNUSED (code already normalizes)

# Cache query embeddings across runs (reserved)
RAG_QUERY_EMBED_CACHE=0  # UNUSED

# R3 — Scoring + multi-query aggregation
# Candidates pulled per query before merging (ACTIVE for Round‑1)
RAG_PER_QUERY_CANDIDATES=80

# Hard cap applied after fusion to prevent runaway result sets
RAG_FUSED_MAX_RESULTS=200

# Similarity metric (reserved; current implementation is cosine via dot-product)
RAG_SIMILARITY_METRIC=COSINE  # UNUSED

# How to merge scores across multiple queries (reserved)
#   max | sum | rrf
RAG_AGG_METHOD=MAX  # reserved knob; Round-1 fusion currently behaves like MAX

# RRF parameters (reserved; only when RAG_AGG_METHOD=RRF)
RAG_RRF_K=60  # UNUSED
RAG_RRF_WEIGHT_MODE=EQUAL  # UNUSED (EQUAL | WEIGHTED)

# Optional per-query weights (reserved; only for SUM/WEIGHTED-RRF)
RAG_QUERY_WEIGHT_1=1.0  # UNUSED
RAG_QUERY_WEIGHT_2=0.8  # UNUSED
RAG_QUERY_WEIGHT_3=0.6  # UNUSED
RAG_QUERY_WEIGHT_4=0.4  # UNUSED

# Diversity controls (reserved)
#   none | page_diverse
RAG_DIVERSITY_MODE=NONE  # UNUSED

# R4 — Filtering + constraints (lexical gates)
# These are applied after vector scoring, before excerpt budgeting.
# Mode: none | any | all
RAG_KEYWORD_MODE=NONE

# Comma-separated required keywords (UNUSED unless RAG_KEYWORD_MODE != NONE)
RAG_REQUIRE_KEYWORDS=

# Comma-separated excluded keywords (UNUSED unless RAG_KEYWORD_MODE != NONE)
RAG_EXCLUDE_KEYWORDS=

# Optional page allow/deny lists (reserved)
RAG_PAGE_WHITELIST=  # UNUSED
RAG_PAGE_BLACKLIST=  # UNUSED

# Optional preference for page-header chunks (reserved)
RAG_PAGE_HEADER_BIAS=0  # UNUSED

# R5 — Evidence budgeting + prompt composition (reserved additions)
RAG_EXCERPT_FORMAT=COMPACT  # UNUSED
RAG_INCLUDE_SCORES=0  # UNUSED
RAG_INCLUDE_METADATA=0  # UNUSED
RAG_PROMPT_TEMPLATE=DEFAULT  # UNUSED

# Timeouts (seconds)
PLANNER_TIMEOUT=0
ANSWER_TIMEOUT=0

# ----------------------------------------------------------------------------
# Streaming / progress reporting
# ----------------------------------------------------------------------------
# STREAM_PROGRESS_SECONDS controls ALL streaming behavior across the project.
# Applies to:
#   - scripts/run_query.py (wrapper)
#   - scripts/answer_with_evidence.py (engine)
#
# Set to >0 to enable streaming with additive rolling progress dumps every N seconds.
# Partial output is preserved and printed even if a request times out.
# Set to 0 to disable streaming and use fully blocking requests.
# Set to 30 during debugging for rolling progress dumps.
STREAM_PROGRESS_SECONDS=0

# Retry behavior
RETRIES=1  # example default; set to 0 to disable retries

# ----------------------------------------------------------------------------
# Token limits (env-controlled; overrides ask_rag.py defaults)
# ----------------------------------------------------------------------------
# Max tokens requested for the planner step
PLANNER_MAX_TOKENS=1800
# Max tokens requested for the refine step (0 = omit max_tokens entirely)
REFINE_MAX_TOKENS=1800

# Max tokens requested for the answer step
ANSWER_MAX_TOKENS=2500
#
# PLANNER_PROMPT_PATH for planner stage system prompt (engine: scripts/answer_with_evidence.py)
PLANNER_PROMPT_PATH=config/planner_prompt.txt  # relative to PROJECT_ROOT

PLANNER_TEMPERATURE=0.7

REFINE_TEMPERATURE=0.6

ANSWER_TEMPERATURE=0.6



# ----------------------------------------------------------------------------
# Debug / trace instrumentation (optional)
# ----------------------------------------------------------------------------
# Per‑probe retrieval debug (Round‑1 observability)
# How many rows to print per probe
RAG_DEBUG_PER_PROBE_TOP=20

# Characters of content preview per row
RAG_DEBUG_PREVIEW_CHARS=180

# Whether to print dropped (non‑kept) rows as well
# 0 = keep‑only (default), 1 = keep + dropped
RAG_DEBUG_PRINT_DROPPED=0

# Enable detailed phase-by-phase tracing in ask_rag.py (planner, retrieval, answer).
# 0 = off (default), 1 = on
RAG_TRACE=0

# Maximum number of characters captured per prompt / excerpt in trace logs
# Helps prevent massive trace files while preserving context
RAG_TRACE_MAX_CHARS=60000

# Directory (relative to PROJECT_ROOT) where trace logs are written
RAG_TRACE_DIR=out/traces